{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to rank on top of Azure Cognitive Search\n",
    "\n",
    "This notebook showcases how to train an L2 ranker, using a [Learn to rank](https://en.wikipedia.org/wiki/Learning_to_rank) approach, to be run on top of Azure Cognitive Search. \n",
    "\n",
    "Through this experiment, we are going to:\n",
    "1. Use Azure Cognitive Search's new feature computation capability to extract text-based similarity features that describe query-to-document relationships\n",
    "2. Do additional feature engineering to enhance our dataset further \n",
    "3. Train a model using [XGBOOST](https://xgboost.readthedocs.io/en/latest/)\n",
    "4. Evaluate the ranking produced by the trained model against the base Azure Cognitive Search ranking using the [NDCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "\n",
    "This experiment uses a dataset containing **7102 articles** from the **docs.microsoft.com** website. Each article contains a title, body, description, list of api names and a url path. Articles were augmented using the [key phrase extraction cognitive skill](https://docs.microsoft.com/en-us/azure/search/cognitive-search-skill-keyphrases) as well as with popular search terms that led to those articles. Additionally, we augmented the articles with easy to compute metadata that will be leveraged when training, such as the number of sections and tables in each article, as well as the normalized page views count.\n",
    "\n",
    "You can find the full index definition at `azs_helpers/index_schema/docs-multilingual-20200217.json`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"docs-multilingual-20200217\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"facetable\": true,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "The experiment also relies on a labeled training set containing over 900 unique queries evaluated against various articles. We refer to this data as the \"judgment\" list, which will be used as the ground truth when evaluating ranking. Each query is evaluated against 1 to 10 different documents, and for each, provides a \"grade\" representing how relevant that specific document is to that query. A value of 10 indicates high relevance, while a value of 1 indicates lower relevance.\n",
    "\n",
    "The judgment list can be found at `data/raw/msft_docs_labels.csv`\n",
    "\n",
    "#### Configuration\n",
    "Details and secrets about your search service should be added to a `config.json` file of this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"service_name\": \"YOUR_SERVICE_NAME\",\n",
    "   \"endpoint\": \"https://YOUR_SERVICE_NAME.search.windows.net\",\n",
    "   \"api_version\": \"2019-05-06-preview\",\n",
    "   \"api_key\": \"YOUR_API_KEY\",\n",
    "   \"index_name\": \"msft-docs\"\n",
    "}\n",
    "```   \n",
    "\n",
    "The file should be located at `config/config.json`  within the repository. If you prefer, you can uncomment (`Ctrl + /`) & fill in the cell below, and it'll generate a service config for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import azs_helpers.l2r_helper as azs\n",
    "from azs_helpers.azure_search_client import azure_search_client as azs_client \n",
    "\n",
    "service_metadata_config_path = Path.cwd() / 'config' / 'config.json'\n",
    "azs_service = azs_client.from_json(service_metadata_config_path)\n",
    "\n",
    "judgement_file_path = Path.cwd() / 'data' / 'raw' / 'msft_docs_labels.csv'\n",
    "judgements = pd.read_csv(judgement_file_path).drop(['title'], axis=1)\n",
    "\n",
    "# index creations and ingestion\n",
    "azs_service.create_index()\n",
    "azs_service.ingest_documents_from_blob_storage(expected_document_count=7102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from search service\n",
    "\n",
    "The following functions are designed to efficiently use the Azure Search service to extract document-query features.\n",
    "\n",
    "1. We filter each queries to only consider the documents we want to judge. This is achieved by adding a **\"filter\"** clause to our search query which will restrict the results to the group of documents contained in the group of documents we have judgment values for.\n",
    "2. We set **\"featuresMode\"** to \"enabled\". This will tell the search service to return additional features with the results, including per-field similarity scores.\n",
    "3. We use the **\"select\"** clause to only return the url of each documents, as well as a few non-text based fields that could potentially be used as features. This will greatly reduce the amount of data that needs to be transfered between the server and the client.\n",
    "4. We use the **\"searchFields\"** parameter to select which text-based fields we want to include in the search process. Those fields will be the only ones for which the service will extract text-based features from (such as per-field similarity).\n",
    "5. We set the **scoringStatistics** parameter to global, which will give us more consistent and predictable scoring by first making sure all index statistics are are \n",
    "\n",
    "The expect response to this query will have the following format:\n",
    "\n",
    "```json\n",
    "    \"value\": [\n",
    "     {\n",
    "        \"@search.score\": 5.1958685,\n",
    "        \"@search.features\": {\n",
    "            \"description_en_us\": {\n",
    "                \"uniqueTokenMatches\": 1.0,\n",
    "                \"similarityScore\": 0.29541412\n",
    "            },\n",
    "            \"body_en_us\": {\n",
    "                \"uniqueTokenMatches\": 3.0,\n",
    "                \"similarityScore\": 0.36644348400000004\n",
    "            },\n",
    "            \"keyPhrases_en_us\": {\n",
    "                \"uniqueTokenMatches\": 3.0,\n",
    "                \"similarityScore\": 0.35014877\n",
    "            },\n",
    "            \"title_en_us\": {\n",
    "                \"uniqueTokenMatches\": 3.0,\n",
    "                \"similarityScore\": 1.75451557\n",
    "            },\n",
    "            \"urlPath\": {\n",
    "                \"uniqueTokenMatches\": 2.0,\n",
    "                \"similarityScore\": 1.07175103\n",
    "            },\n",
    "            \"searchTerms\": {\n",
    "                \"uniqueTokenMatches\": 3.0,\n",
    "                \"similarityScore\": 1.3575956200000001\n",
    "            }\n",
    "        },\n",
    "        \"normalized_pageview\": null,\n",
    "        \"tableCount\": 0,\n",
    "        \"sectionCount\": 7,\n",
    "        \"url_en_us\": \"https://docs.microsoft.com/en-us/azure/search/\"\n",
    "    }]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results_from_service(service, query, urls_filter):\n",
    "    search_request_body = {\n",
    "        \"search\": azs.escape_query(query),\n",
    "        \"featuresMode\": \"enabled\",\n",
    "        \"select\": \"title_en_us, url_en_us, sectionCount, tableCount, normalized_pageview\", \n",
    "        \"searchFields\": \"body_en_us,description_en_us,title_en_us,apiNames,urlPath,searchTerms, keyPhrases_en_us\",\n",
    "        \"scoringStatistics\": \"global\",\n",
    "        \"top\" : 20\n",
    "    }\n",
    "    if len(urls_filter) > 0:\n",
    "        search_request_body[\"filter\"] = \" or \".join(f\"url_en_us eq '{url}'\" for url in urls_filter)\n",
    "\n",
    "    return service.search(search_request_body)\n",
    "\n",
    "def get_features_from_service(service, query, group):\n",
    "    urls = group['url'].values.tolist()\n",
    "    \n",
    "    search_results = get_search_results_from_service(service, query, urls)\n",
    "\n",
    "    # this will flatten the search json response into a panda dataframe\n",
    "    azs_features = pd.json_normalize(search_results)\n",
    "    \n",
    "    # we add the data extracted from azure search to our labeled data by merging them on the \"url\" field\n",
    "    merged_results = group.join(azs_features.set_index('url_en_us'), on='url')\n",
    "    \n",
    "    # some of the labeled documents in our dataset did not match any documents in the Azure Search instance,\n",
    "    # we will remove them from our data by dropping any row that did not produce a search score\n",
    "    return merged_results.dropna(subset=['@search.score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make parallel calls to the Azure Search service to extract features\n",
    "\n",
    "To extract all the features from our dataset, we start by grouping the judgment list by query. This will provide us with a list of judged documents for each query. Each call to the Azure Cognitive Search service will use the query from the group,  with filters to make sure we only return the documents from the group. In this dataset, we can expect aproximately 900 queries.\n",
    "\n",
    "To quickly execute those queries, we setup a thread pool executor which will run the queries in parallel. The level of parallelism can be changed to accomodate different search service capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import concurrent\n",
    "import datetime\n",
    "from itertools import chain\n",
    "\n",
    "query_groups = judgements.groupby('query')\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(30)\n",
    "futures = [executor.submit(get_features_from_service, azs_service, query, group) for (query, group) in query_groups]\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "all_features = pd.concat([future.result() for future in futures if future], sort=False).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize our Dataset\n",
    "\n",
    "Now that the data has finished downloading, let's load our data. Pay attention to a few things in particular:\n",
    "\n",
    "- **grade** is the ground truth relevance score for this document, where a grade of '10' is most relevant, while a grade of '1' is least relevant. It is an [ordinal variable](https://en.wikipedia.org/wiki/Ordinal_data) ranging from **1 ~ 10**, from least relevant to most relevant.\n",
    "- **title_en_us** is the English-language title of an article on Microsoft Docs.\n",
    "- **query** is the search query that led to this document\n",
    "- All features that begin with an \"@\" character are returned from the Features API, and will be used extensively by the trained model.\n",
    "- Some queries have less than 10 documents. We'll have to keep this fact in mind when preparing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_groups = all_features.groupby('query')\n",
    "docs_per_query = query_groups.size()\n",
    "print(f\"Number of unique queries: {len(all_features.groupby('query'))}\")\n",
    "print(f\"Number of documents: {len(all_features)}\")\n",
    "print(f\"Number of features: {len(all_features.columns)}\")\n",
    "print(f\"Fewest docs in a single query: {docs_per_query.min()}\")\n",
    "print(f\"Most docs in a single query: {docs_per_query.max()}\")\n",
    "\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "In this section, we will apply additional domain knowledge to add a few features to our model. For example, we are going to add the title length, in words and in characters.\n",
    "\n",
    "We are also going to drop columns of our dataset that we won't be using for the training process, such as raw url, title and query text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def customize_features(features):\n",
    "    # adding a query_id column will help us group the data per query later in training\n",
    "    if 'query_id' not in features:\n",
    "        features['query_id'] = features.groupby('query').ngroup()\n",
    "    \n",
    "    if ('title_en_us' in features):\n",
    "        features['title_en_us'] = features['title_en_us'].map(lambda title: str(title))\n",
    "        features['title_length_in_words'] = features['title_en_us'].map(lambda title: float(len(title.split())))\n",
    "        features['title_length_in_chars'] = features['title_en_us'].map(lambda title: float(len(title)))\n",
    "\n",
    "    # Discard the columns that we no longer need\n",
    "    features = features.drop(['query', 'url', 'title_en_us'], axis=1, errors='ignore')\n",
    "    return features\n",
    "\n",
    "all_features = customize_features(all_features)\n",
    "print(\"All features collected:\")\n",
    "pprint(all_features.columns.values)\n",
    "\n",
    "all_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Azure Cognitive Search NDCG\n",
    "\n",
    "[**Normalized Discounted Cumulative Gain (NDCG)**](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Discounted_cumulative_gain) is a common metric used to evaluate search ranking systems.  Using `@search.score` returned from the service, we can calculate and visualize Azure Search's NDCG. This will be the baseline to beat with our ranking model.\n",
    "\n",
    "- `NDCG@K` denotes the NDCG for lists of K length. For example:\n",
    "    - `NDCG@1` denotes calculating NDCG for a list containing only the top result in a search query\n",
    "    - `NDCG@5` would refer to the NDCG for a list containing the top 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "query_groups_with_azs_ranking = all_features.sort_values(['@search.score'], ascending=False).groupby('query_id')\n",
    "ndcg_inputs = [[doc['grade'] for _, doc in group.iterrows()] for query_id, group in query_groups_with_azs_ranking]\n",
    "original_ndcg = azs.evaluate_ndcg(k_start=1, k_end=10, baseline=ndcg_inputs, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization\n",
    "\n",
    "Similarity scores are computed to be compared within the context of a single search query, where all documents are evaluated against the same search terms. The range of those scores fluctuates depending on what terms are being searched (for example, common terms often yield to lower similarity scores, since their document frenquency is higher).\n",
    "\n",
    "To address this problem, we are going to normalize most of our features using the [minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) scaler, which will be applied at the query level group, so that all documents associated with a particular query will have their feature values scaled from 0 to 1.\n",
    "\n",
    "For features with outlier values, such as `normalized_pageview`, we will use the [QuantileTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale, QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "def normalize_features(features):\n",
    "    query_groups = features.groupby('query_id')\n",
    "\n",
    "    for feature in features.columns:\n",
    "        if feature not in['normalized_pageview', 'grade', 'query_id'] and features.dtypes[feature] in [float, int]:\n",
    "            features[feature] = query_groups[feature].transform(lambda x: minmax_scale(x.astype(float)))\n",
    "            \n",
    "    if 'normalized_pageview' in features.columns:\n",
    "        features['normalized_pageview'] = query_groups['normalized_pageview'].transform(\n",
    "            lambda x: np.concatenate(\n",
    "                QuantileTransformer(n_quantiles=min(len(x), 4)).fit_transform(x.values.reshape(-1,1))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return features\n",
    "    \n",
    "df = normalize_features(all_features)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize Data for our Experiment\n",
    "\n",
    "Let's serialize our data to disk. The tutorial will continue in [Part 2: Experiment](./l2r_part2_experiment.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_data_dir = Path.cwd() / 'data' / 'interim'\n",
    "interim_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(interim_data_dir / 'normalized_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
