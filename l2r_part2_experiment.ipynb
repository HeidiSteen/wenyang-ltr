{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "Let's load the data and configs from the previous notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "import azs_helpers.l2r_helper as azs\n",
    "from azs_helpers.azure_search_client import azure_search_client as azs_client \n",
    "\n",
    "interim_data_dir = Path.cwd() / 'data' / 'interim'\n",
    "df = pd.read_csv(interim_data_dir / 'normalized_features.csv')\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into features and judgments\n",
    "\n",
    "The machine learning libraries we're using expect us to separate our training data into features $X$ and judgment values $y$. Let's do that real quick: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['grade', 'query_id'], axis=1), df.grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold out a portion of the data for testing.\n",
    "\n",
    "We need to hold out part of the data to validate that our model works.\n",
    "\n",
    "For this tutorial, we will hold out **10%** of the data using `scikit-learn.model_selection.GroupShuffleSplit`. This means that the model will use **90%** of the queries for training & validation, while **10%** of the queries (that the model has never seen before) will be used to evaluate the final model's performance.\n",
    "\n",
    "#### Query labels for documents\n",
    "\n",
    "Our data preparation and model training process must be aware of query-document relationships in our dataset for accurate training results, so we'll need to pass in the query labels (`query_ids`) as well. The reason this is important is because the loss function used for optimization depends on the NDCG. As discussed above, the NDCG is a metric which indicates how close is an ordered list to its ideal order. To achieve this, the model needs to be aware of how to create those lists out of the training set, which can be achieved by grouping the data using the query_id.\n",
    "\n",
    "Read the following references for more information:\n",
    "\n",
    "- [Scikit-Learn: Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Scikit-Learn: GroupShuffleSplit](https://scikit-learn.org/stable/modules/cross_validation.html#group-shuffle-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Hold out 10% of our queries in the test set.\n",
    "random_seed = 42 # we are using a static random seed so we can easily reproduce results. The actual number has no meaning.\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# Query labels for each document in our dataset.\n",
    "query_ids = df['query_id'].to_numpy()\n",
    "\n",
    "train_index, test_index = next(gss.split(X, y, groups=query_ids))\n",
    "print(f\"Number of samples in train: {len(train_index)}\")\n",
    "print(f\"Number of samples in test: {len(test_index)}\")\n",
    "\n",
    "# Split our features, judgment values, and query labels into train & test sets.\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "query_ids_train, query_ids_test = query_ids[train_index], query_ids[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "Now that we've completed our feature engineering steps, we can move on to running our experiment. We'll be using the open-source [XGBoost library](https://xgboost.readthedocs.io/en/latest/index.html), which includes a few implementations of [**Pairwise**](https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach) and [**Listwise**](https://en.wikipedia.org/wiki/Learning_to_rank#Listwise_approach) ranking algorithms.\n",
    "\n",
    "By setting `'objective': 'rank:ndcg'` in the model constructor, we've chosen to use the [LambdaMART](https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/) algorithm to maximize **listwise** [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain).\n",
    "\n",
    "Another objective function that can be used for this problem is `rank:pairwise`, which will perform **pairwise** training instead of listwise training.\n",
    "\n",
    "### Further Reading:\n",
    "- [Learning to Rank: From Pairwise Approach to Listwise Approach](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)\n",
    "- XGBoost Documentation links:\n",
    "    - [XGBRanker Class](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRanker)\n",
    "    - [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {'objective': 'rank:ndcg', 'learning_rate': 0.1,\n",
    "          'gamma': 1.0, 'min_child_weight': 0.1,\n",
    "          'max_depth': 8, 'n_estimators': 500}\n",
    "\n",
    "ranker = xgb.XGBRanker(**params)\n",
    "ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with K-Fold Cross Validation\n",
    "\n",
    "K-Fold cross validation is a robust way to avoid overfitting during model training. Let's train with K-fold cross validation with `K=9`. We'll be using `scikit-learn`'s [GroupKFold class](https://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold) to split our training data into 9 approximately even-sized portions. `GroupKFold` ensures that all documents for a particular query remain within the same fold. Before we can get started, we'll need to augment our query labels for training.\n",
    "- If you're running this on Binder, set`n_splits` = 2 instead.\n",
    "\n",
    "#### Computing Document counts per Query\n",
    "\n",
    "During the training phase, we need to transform our query labels to be compatible with XGBoost. The `GroupKFold` cross-validation tool provided by `scikit-learn` expects a group (or query) label for every document in the dataset. We've already provided this in the form of `query_ids`.\n",
    "\n",
    "Let's take a look at the example below. Given a simple dataset of 3 queries:\n",
    "\n",
    "- Query 1: [doc1, doc2, doc3]\n",
    "- Query 2: [doc4]\n",
    "- Query 3: [doc5, doc6]\n",
    "\n",
    "**`scikit-learn` expects the following format**:\n",
    "\n",
    "```python\n",
    "# query_ids = [query1, query1, query1, query2, query3, query3]\n",
    "query_ids = [1, 1, 1, 2, 3, 3]\n",
    "```\n",
    "\n",
    "**`xgboost` expects the following format**:\n",
    "\n",
    "```python\n",
    "query_doc_counts = [3, 1, 2]\n",
    "```\n",
    "    \n",
    "`xgboost` expects document counts for each query (as opposed to a query label for every document), sorted by query. We can take our scikit-learn compatible query labels and aggregate them using numpy, as follows:\n",
    "\n",
    "```python\n",
    "labels, query_doc_counts = np.unique(query_ids, return_counts=True)\n",
    "```\n",
    "\n",
    "or, alternatively:\n",
    "```python\n",
    "query_doc_counts = np.unique(query_group_labels, return_counts=True)[1]\n",
    "```\n",
    "\n",
    "For reference, please refer to the [XGBoost Docs](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRanker)\n",
    "\n",
    "### Calculating our Results\n",
    "Calling `ranker.predict(X_test)` here will allow us to get all of the results. Since `xgb_scores` is a flat list of scores that aren't grouped by query, we'll need to slice it with sliding windows to create predicted rankings for each query. Feel free to read the docstring of `azs_helpers.l2r_helper.compute_judgments_after_training` for more information.\n",
    "\n",
    "We expect uneven group sizes like the following:\n",
    "\n",
    "- Query 1: 10 search results\n",
    "- Query 2: 8 search results\n",
    "- Query 3: 7 search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def calculate_ndcg_for_experiment(ranker, X, y, query_ids):\n",
    "    n_splits = 9\n",
    "    group_kfold = GroupKFold(n_splits)\n",
    "\n",
    "    ndcg_results = []\n",
    "\n",
    "    for train_idx, validation_idx in group_kfold.split(X, y, groups=query_ids):\n",
    "        X_train_fold, X_validation_fold = X.iloc[train_idx], X.iloc[validation_idx]\n",
    "        y_train_fold, y_validation_fold = y.iloc[train_idx], y.iloc[validation_idx]\n",
    "        \n",
    "        # XGBoost expects query counts instead of query labels.\n",
    "        # https://stackoverflow.com/questions/10741346/numpy-most-efficient-frequency-counts-for-unique-values-in-an-array\n",
    "        train_groups_xgb = np.unique(query_ids[train_idx], return_counts=True)[1]\n",
    "        validation_groups_xgb = np.unique(query_ids[validation_idx], return_counts=True)[1]\n",
    "        \n",
    "        ranker.fit(X_train_fold, y_train_fold, train_groups_xgb,\n",
    "                   eval_set=[(X_validation_fold, y_validation_fold)], eval_group=[validation_groups_xgb],\n",
    "                   eval_metric='ndcg',\n",
    "                   verbose=False)\n",
    "        xgb_scores = ranker.predict(X_validation_fold)\n",
    "        model_judgments, azs_judgments, baseline = azs.compute_judgments_after_training(xgb_scores, validation_groups_xgb, X_validation_fold, y_validation_fold)\n",
    "        ndcg_results_for_fold = azs.evaluate_ndcg(k_start=1, k_end=10, azs=azs_judgments, predicted=model_judgments)\n",
    "        ndcg_results.append(ndcg_results_for_fold)\n",
    "\n",
    "    return ndcg_results\n",
    "\n",
    "# %memit ndcg_results = calculate_ndcg_for_experiment(ranker, X_train, y_train, query_ids_train)\n",
    "ndcg_results = calculate_ndcg_for_experiment(ranker, X_train, y_train, query_ids_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cross-Validation Results\n",
    "\n",
    "Now that we've trained our model multiple times, we can visualize the training results.\n",
    "\n",
    "Datapoints represent the mean across all training runs, while vertical bars are the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "azs.show_ndcg_results(ndcg_results, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now that we're confident that our model performs better than Azure Search's built-in retrieval algorithm, let's run it against the test set! This time around, we'll be using all of the training data available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups_xgb = np.unique(query_ids_train, return_counts=True)[1]\n",
    "test_groups_xgb = np.unique(query_ids_test, return_counts=True)[1]\n",
    "\n",
    "ranker.fit(X_train, y_train, train_groups_xgb,\n",
    "           eval_metric='ndcg',\n",
    "           verbose=False)\n",
    "\n",
    "xgb_scores = ranker.predict(X_test)\n",
    "model_judgments, azs_judgments, baseline = azs.compute_judgments_after_training(xgb_scores, test_groups_xgb, X_test, y_test)\n",
    "\n",
    "test_ndcg_results = azs.evaluate_ndcg(k_start=1, k_end=10, plot=True, show_lift=True, azs=azs_judgments, predicted=model_judgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "We can view which features ended up being the most important in our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = dict(zip(X_train.columns, ranker.feature_importances_))\n",
    "feature_importances_sorted = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse=True)}\n",
    "feature_importances_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to re-rank a search request's results using our trained model\n",
    "When re-ranking an Azure Search query, we need to go through the same process to transform the results into features as we did for our training data. \n",
    "\n",
    "This means\n",
    "\n",
    "1. Get features from Azure Search\n",
    "2. Compute any additional features that was used int raining\n",
    "3. Apply the same level of normalization \n",
    "\n",
    "Once we get the features in the expected format, we can feed them into the trained model to infer a new order, which will be used to re-rank our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_search_query(service, query, model, expected_features):\n",
    "    expected_features_format = pd.DataFrame(columns = expected_features, dtype='float64')\n",
    "    \n",
    "    json_search_results = azs.get_search_results_from_service(service, query, [])\n",
    "    search_results = pd.json_normalize(json_search_results).fillna(0)\n",
    "    search_results[\"query\"] = query\n",
    "\n",
    "    search_features = azs.customize_features(search_results.drop(columns=['url_en_us'], axis=1))\n",
    "    normalized_search_features = azs.normalize_features(expected_features_format.append(search_features))\n",
    "    \n",
    "    prediction_score = model.predict(normalized_search_features.drop(columns=['query_id'], axis=1, errors='ignore'))\n",
    "    search_results['prediction'] = prediction_score\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Base Azure Search order'] = search_results.apply(lambda row: \"{0}\\n{1}\".format(row['title_en_us'], row['url_en_us']), axis=1) \n",
    "    new_df['Trained model order'] = search_results.sort_values('prediction', ascending=False).apply(lambda row: \"{0}\\n{1}\".format(row['title_en_us'], row['url_en_us']), axis=1).tolist() #['title_en_us'].tolist()\n",
    "\n",
    "    return new_df\n",
    "service_metadata_config_path = Path.cwd() / 'config' / 'config.json'\n",
    "azs_service = azs_client.from_json(service_metadata_config_path)\n",
    "azs.pretty_print(rerank_search_query(azs_service, 'Windows Server 2019', ranker, X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
